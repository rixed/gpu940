<html>
<head>
	<title>gpu940 documentation</title>
</head>
<body>
<h1>gpu940 documentation</h1>
<p>
	gpu940 is a soft 3D renderer that can perform true perspective texture 
mapping, lighting, blending, depth tests... Primitives are not limited to 
triangles, and are clipped in 3D before remaining vertexes are projected. It 
was targeted against GPH's <a href="http://wiki.gp2x.org">GP2X</a> but could 
probably be of some use on other platforms as well.
</p><p>
	This document gives some details about the internals of gpu940, in order to 
ease it's use by others as well as to helps new contributors in joining the 
project.
</p>
<h2>Table of content</h2>
<ul>
	<li>
		<a href="#Presentation">Presentation</a>
		<ul>
			<li><a href="#History">History</a></li>
			<li><a href="#Design">Overall design</a></li>
			<li><a href="#SourceCode">Source code structure</a></li>
		</ul>
	</li><li>
		<a href="#GPU">GPU implementation</a>
		<ul>
			<li><a href="#Comm">Communication between CPU and GPU</a></li>
			<li><a href="#Commands">GPU commands</a></li>
			<li><a href="#PolyPersp">True perspective polygon drawing</a></li>
			<li><a href="#PolyNoPersp">No perspective polygon drawing</a></li>
			<li><a href="#YUV">Handling YUV color model</a></li>
			<li><a href="#JIT">"Just In Time" code generation</a></li>
		</ul>
	</li><li>
		<a href="#libgpu">Helper library</a>
		<ul>
			<li><a href="#need4lib">The need for a client library</a></li>
			<li><a href="initialization">Initialization</a></li>
			<li><a href="commandbuffer">Handling the command buffer</a></li>
			<li><a href="videobuffer">Handling the video buffer</a></li>
			<li><a href="synchro">Synchronization with GPU</a></li>
		</ul>
	</li><li>
		<a href="#OpenGL">OpenGL implementation</a>
		<ul>
			<li><a href="#GLMachine">Overall OpenGL machine</a></li>
			<li><a href="#GL2GPU">Binding to the GPU</a></li>
			<ul>
				<li><a href="#GLcoord">Vertex coordinates</a></li>
				<li><a href="#GLcolor">Pixel colors</a></li>
			</ul>
			<li><a href="#GLswap">Swapping buffers</a></li>
			<li><a href="#GLfloats">Other types than GLfixed</a></li>
		</ul>
	</li><li>
		<a href="#App">Appendix</a>
		<ul>
			<li><a href="#AppBrew">How to write new OpenGL apps</a></li>
			<ul>
				<li><a href="#ABConf">Conformance issues</a></li>
				<li><a href="#ABPerf">Performance issues</a></li>
			</ul>
			<li><a href="#AppPort">How to port OpenGL apps</a></li>
		</ul>
	</li>
</ul>
<h2><a name="Presentation">Presentation</a></h2>
<p>
	This part covers the design of gpu940, starting with a time line of the
project that can help understand some design outcome.
</p>
<h3><a name="History">History</a></h3>
<p>
	At first, no OpenGL library was planned. I just wanted to code a polygon
renderer like the many ones we wrote in the good old days of software 
rendering.
</p><p>
	For the occasion, I wanted to give a try to the rendering technique called
"Z-const mapping". More on this later, but the idea is to trade writing
sequentially (cache friendliness) for true perspective.
</p><p>
	Most of the time spent on the whole project was spent on this true
perspective mapping routine. Not that the source code is very big but it
was not easy to debug (rendering glitches in general are hard to debug,
because the glitch often does not belong to a given pixel in isolation
but to the whole set of pixels that gives a picture, and if you are unlucky
even a single picture is not false by itself but the animated polygons are
still wrong).
</p><p>
	This true perspective mapping routine seemed to suit perfectly the GP2X's
940 CPU because of it's little data cache. This is when the project grown
from a draw_polygon function to a complete software GPU (that I will call
"GPU" from now on for short).
</p><p>
	Also, an option was added to lighten or darken each pixels according to
a per-vertex light parameter. Notice how this is different from OpenGL
lighting, which computes resulting colors of vertexes first, then interpolate
this colors through the polygon. OpenGL lighting allows for more complex
lighting (colored light sources, complex lighting model) but requires to
interpolate the three color components, where my little technique needed only
one parameter, but permitted only the simplest of all lighting model (color +
light). If I had though that I was about to implement an OpenGl layer on top
of this set of functions, of course, I would have made other choices.
</p><p>
	After some run of the test programs on the GP2X (most of the tests were
run on the PC version), it appeared that lighting in the 16 bits RGB model
was not satisfactory&nbsp;: large light strips could be seen where the 
lighting parameter flipped a color bit. That's when I envisaged YUV mode, 
which is 24 bits. The strips almost vanished so that change was kept despite 
there is no
YUV mode in SDL for the PC version. It was then required to convert all
individual RGB colors and textures to the color model used by the GPU (RGB
on PC, YUV on GP2X).
</p><p>
	The "Code alone" little demo was done at that point, to prove the usability
of the GPU. It also revealed that accessing the memory is not as fast as
expected. It was discovered then that accessing the RAM with big gaps between
addresses, which is exactly how the true perspective renderer writes its 
pixels, leads to very long RAM delays. This seems to be specific to the GP2X 
design rather that to the ARM architecture. Z-constant mapping was no more 
adapted to every polygons, but should be reserved for large textured polygons 
when the perspective matter. For all others, a simpler, traditional rasterizer 
should be used, which writes pixels sequentially in a scan line. This, also, 
would offer another source of optimisation&nbsp;: writing several pixels in 
one go with the ARM stm instruction, which further reduces RAM latency.
</p><p>
	After some bench, it appeared that only half the time spent in drawing a
polygon is spent "productively" inside the function that actually writes
pixels. The rest is spent evaluating input parameters, computing rendering
parameters, etc... For a quick and dirty (and hopefully, temporary) 
improvement on this, the polygon function was split between perspective and 
no-perspective (thus we avoid many of the if/else tests), although initially 
only the writing functions were different.
</p><p>
	Then came the idea of an OpenGL-ES implementation, at first as a way to
allow homebrew coders to create new apps that they could then keep with them
as they move on other platforms. In addition, OpenGL-ES has a well-known API
that would dispense gpu940 of a proper documentation. Of course this require
z-buffer, which was not implemented by then, but was planned anyway. And this
API would prevent the use of some fancy features of gpu940 (like mixing
all buffers indifferently).
</p><p>
	While adding an OpenGL-ES library, I looked for a simple OpenGL-ES 
application to port on GP2X for testing purpose. I then realized how small 
OpenGL-ES audience is compared to mere OpenGL.  Also, many OpenGL programs 
these days are really not demanding much to the GL driver&nbsp;: feature them 
with a z-buffered triangle mapping function, and most of there needs are fed 
(because this is what is actually optimized by the cheap hardware available).
That's when I decided to give a try to plain OpenGL&nbsp;: to find an app to 
port
on the GP2X to test gpu940's OpenGL-ES implementation, which was there to
help coders to port their apps out to other platforms. How strange.
</p><p>
	Implementing OpenGL means that instead of having 5 or 6 pre-defined 
rendering functions to choose from, the user has access to a whole set of 
parameters that together define a rendering functions&nbsp;: z-test modes, 
write bit mask, texture, texture modes, color, alpha tests, blending, 
interpolation modes, etc... A general purpose function implementing all the 
possible cases would be crowded with tests and thus would be very slow. A 
better approach consist of generating the rendering function corresponding to 
these parameters values. Vincent-gl does the same thing, and probably MESA too 
in some extents.  I decided to do the same, so I removed all the pre-defined 
rendering functions and replaced them by a kind of "just in time compiler" for 
ARM (very presumptuous name for a routine that almost merely concatenate 
predefined assembly snipets), and a general purpose C function for the PC.
</p><p>
	At the same time, I started to realize that the real value of gpu940 was to 
implement OpenGL, so I modified several specific rendering methods which were 
unusable from OpenGL, and added some more costly rendering modes needed by 
OpenGL (eg. shadows rendering became blending).
</p><p>
	I also started to look for a proof of concept to demonstrate the ability of 
the GPU. I had to port something, preferably a well known OpenGL game, simple 
enough to not knock down the GPU on its knees, because the only OpenGL 
application I ported thus far, glxgears, run only at around 15 fps. I first 
tried glBattalion, but despite being visually poor, this game make use of a 
large amount of distinct OpenGL API services that are seldom used and that I 
didn't want to implement just for this demonstration. That's when I had a look 
to Egoboo's source code. Like recent games, Egoboo is designed to use only the 
bare minimum of OpenGL (actually, it's a Direct3D port), that is a polygon 
mapping rasterizer with z-buffer and blend. It only took minutes to obtain the 
first version compiled with gpu940's libGl. Of course it revealed many bugs 
and required many other modifications than just linking with gpu940, but this 
is another story.
</p><p>
	From this, the future of gpu940 appears to be closer and closer from OpenGL 
requirements. Rendering commands should match OpenGL commands in order to 
minimize libGL wrapping task on the client (the 920), for example for blending 
and alpha support. In parallel, polygon shaper must be made more accurate. In 
the speed front (speed also means power saving), the polygon shaper must be 
made faster, as it can eat as much as 20% of the rendering time.
</p><p>
	When those objectives will be met, well, what should be done would have 
been done.
</p>
<h3><a name="Design">Overall design</a></h3>
<p>
	The software is designed to operate in the background, in a distinct thread 
of control than its client, much like a hardware GPU. It's not, however, 
designed to serve several clients (an OpenGL feature we really won't need). On 
PC, this means running on a distinct process, using IPC mechanisms to 
communicate with client. On the GP2X, this means running on the 940.
</p><p>
	Commands are sent to the GPU using a cyclic buffer. On PC, this buffer is 
mmapped and shared by the GPU and the client. On the GP2X, the buffer is 
located on a dedicated portion of the upper 32Mb. This design free us from the 
need to synchronize the two threads of controls with complex semaphore, as the 
client only writes to the buffer and to the end pointer and only reads the 
begin pointer, and the GPU only reads the buffer and end pointer and writes 
the begin pointer. The only other information shared by the GPU and its client 
are some error flags and statistical counters that are atomically written by 
GPU and read by the client. Also, the video buffers are accessible by both the 
GPU and the client, which can then upload directly textures or read previously 
rendered pictures. The whole used memory is sized according to the GP2X, and 
spawn along the whole 32 Mb of upper memory, minus 64Kb for gpu940's code and 
stack. Amongst that, 1 Mb is used for the command buffer&nbsp;: this is enough 
to store approximately 5000 polygon commands, which is more than most frames 
require to be rendered.
</p><p>
	To render 3D pictures, you need computing vertexes in the camera space,
projecting, clipping, and drawing. One must choose what's performed by the 
GPU, and what's left for the client. The choice is a matter of trading 
simplicity of use for freedom for the client to do what it wants the way it 
wants to. It's also, for a soft renderer, a matter of sharing the load on two 
equivalent processors so that no one has to continuously waits for the other.  
Because it seemed not necessary to constraint the client to use some given 
method to compute it's geometry positions, and because I know and enjoy many 
cheats for that, and because there exists many different arguable methods to 
handle primitive transformations, I decided to let the client handle the 
camera and vertex positioning. Otherwise, the GPU needs to handle matrix 
stacks, space partitionning, different spaces scales, characters skinning, and 
many things that's not the business of a renderer (in addition, the client 
will need the resulting positions for it's own stuff). However, for 
true-perspective and z-tests, the GPU needs to know those resulting positions, 
so it was better to let it project and clip the polygons itself. So the client 
must send 3D vertexes in camera space, and the gpu940 handle all the 3D to 2D 
projection and clipping. The drawback is that it's no more possible for the 
GPU to share a vertex between several polygons&nbsp;; each vertex is then projected 
each time it's encountered. To minimize this cost, which still remains 
important, the GPU maintain a cache of projected vertexes and the client has a 
way to tell it that a given vertex is the same than a previously sent
one.
</p><p>
	For clipping, you can choose to clip the 3d coordinates against the view 
frustum, or the 2D coordinates against the border of the rendering window.  
Clipping in 3D means you only need to project the vertex you will use for 
rendering, while clipping in 2D is generally considered simpler. gpu940 clips 
in 3D, in order to minimize the amount of required divisions and because it's 
actually simpler. Clipping in 3D also offer the possibility to have user 
clip-planes. Actually, the frustum is just a set of predefined clip-planes 
(the code alone demo uses this to clip the "gpu940" logo shadow against the 
border of the surrounding cube, which is much faster than using a z-test).
</p><p>
	Hardware GPU often have different kind of RAM, one which is read for texel 
values, and another one where pixels, Z values or alpha values are written.  
For a software renderer we do not have to distinguish between the two. It 
seemed simpler and more powerful to treat the whole video buffer indifferently 
from the GPU, and let the client decide where the GPU should draw its pixels, 
where it should read texels, where it should read and write Z values, etc...  
The drawback is that the client must then handle the video buffer itself, 
allocating space for textures, pictures and Z-bufers, freeing them when 
necessary. But it allows the client, for example, to simply render-to-texture, 
or use a previously rendered picture or a texture as a Z-buffer, etc... It 
also allows it to use more than two video buffers (the code alone demo uses 
this to render many frames in advance when the GPU is faster than the frame 
rate, so that it can save some time to render more demanding frames).  
Hopefully, a library helps the client to manage the video buffer.
</p><p>
	ARM940 comes with no FPU (nor does ARM920). We then must use fixed point 
arithmetic all along (which is a good thing for a 3D renderer anyway, because 
all manipulated values have the same scale). The simplest fixed point format 
possible on a 32bit machine is 16.16 (16 bits for integer part and 16 bits for 
decimal part). That's what will be used. A helper library will provide the GPU 
with arithmetic helper functions (most of them inline), also linkable to the 
client. These include multiplication, division, sign comparison and
absolute value, division being the one that must be avoided at all cost. For 
convenience for the client, we also added square root,
sine and cosine, although not required by the GPU.
</p><p>
	A word on true-perspective. Perspective distortion is mainly a problem for 
texture mapping, but this issue affects all rendering techniques other than 
constant color ("flat") rendering. The same z-constant technique can be 
applied to any parameter that must be interpolated in 3D space along the 
polygon at no cost&nbsp;: once the z-constant "scanline" is found, any 
parameter can be linearly interpolated, not only texture coordinates U and V.  
From this remarks, we can deduce that no parameter play a special role in the 
polygon shaper. Parameters are only meaningful for the rasterizer (the 
function that draw a "scanline"). Until there, parameters are just a set of 
values we want to interpolate on 3d space. In all other places of GPU code, we 
only need to know how many parameters we have, which is given by the polygon 
command sent by the client.
</p><p>
	Finally, 3D renderers comes in two flavors&nbsp;: the clumsy ones that can 
draw only triangles, and the ones that can draw any (convex) polygons. I can 
think of no valid reason why the GPU should be limited to triangles.
</p>
<h3><a name="SourceCode">Source code structure</a></h3>
<p>
	The GPU and the user application share a common data structure in memory 
named <i>shared</i> of type <i>struct gpuShared</i>, which main members are 
the <i>uint32_t</i> arrays <i>cmds</i>, the command buffer, and 
<i>buffers</i>, the video buffers. This structure, as well as all available 
commands and helper library calls, are defined in <i>gpu940.h</i>.
</p><p>
	Commands are small data structures of which the first word gives the 
operation code. The main command is the <i>draw polygon</i> command, which 
gives all rendering information and the 3D coordinates of all involved 
vertexes.  When it encounters this command, the GPU first clips the polygon, 
projects vertexes and draws the polygon onto the current <i>out</i> buffer.  
There are no camera nor <i>modelview</i> matrix stack as far as the GPU is 
concerned&nbsp;; all 3D transformations must have been performed by the user 
application before sending the draw command. Other commands do change viewer 
window coordinates, z-test mode, define buffers, etc...
</p><p>
	Buffers are defined by the <i>struct buffer_loc</i> data type, which 
consist of the buffer starting address (offset, in words, into video buffers), 
width (constrained to a power of two) and height. This struct is sent within 
the command that sets GPU buffers.
</p><p>
	GPU uses three distinct buffers&nbsp;:
</p><ul>
	<li>the <i>out</i> buffer, into which everything is drawn&nbsp;;</li>
	<li>the <i>txt</i> buffer, from which texels are fetched&nbsp;;</li>
	<li>the <i>z</i> buffer, where z values are stored and read to perform z 
filtering&nbsp;;</li>
</ul><p>
	When all commands of a frame are sent, the user application send a <i>show 
buffer</i> command to publish the frame. This command, when executed by the 
GPU, means to push the designated buffer to the list of displayable buffers 
(<i>struct buffer_loc</i> array named <i>displist</i>). Each time a vertical 
interrupt occur (or, on PC, a timer interrupt), the GPU takes the next buffer 
from this list and make it the current displayed buffer. If the list is empty, 
the interrupt is <i>missed</i> (missed frame count is maintained in the 
<i>shared</i> structure, and displayed on the OSD), meaning that there were 
too many commands to perform in one frame time. When, at the contrary, more 
than one frames are published during one frame time, the GPU will display them 
one at a time, synchronized with the vertical interrupt&nbsp;; in other words, it 
will not skip frames. For synchronisation purposes, the user application can 
read the frame counters <i>frame_count</i> and <i>frame_miss</i> in the 
<i>shared</i> structure that account for each displayed frame and missed 
interrupt.
</p><p>
	There is a distinct header file named <i>fixmath.h</i> where are grouped 
all fixed point maths functions. Both <i>gpu940.h</i> and <i>fixmath.h</i> are 
used by the GPU code and the client code, and thus are located in the 
<i>include</i> directory.
</p><p>
	gpu940 code is located in the <i>bin</i> directory. <i>crt0.S</i> is the 
only assembly file, and holds the interrupt vector, the initial setup code, 
and some functions like division. Its main purpose is to setup the ARM940 
protection unit&nbsp;; it is not used when building for PC. The protection unit is 
configured for cache everything except the command buffer (and, of course, 
MMSP's IO register set).
</p><p>
	gpu940 code flow then follows in <i>gpu940.c</i>, which handle command 
reading main loop, MMSP settings, vertical IRQ, and serve all commands but
the draw polygon command.
</p><p>
	This one is split into various files. First, clipping and projection is 
done in <i>clip.c</i>, and if something is left to be displayed, polygon 
shaping is performed in <i>poly.c</i>, or <i>poly_nopersp.c</i> if the client 
asked for linear interpolation of all parameters. Then each scan line (or 
z-const line) is drawn in <i>raster.c</i> on PC, and by the code generated by 
<i>codegen.c</i> on ARM.
</p><p>
	The <i>lib</i> directory holds all source files that together form the 
helper library.
</p><p>
	The <i>GL</i> directory stores all source code for the <i>libGL</i>, while 
the GL header files are located in <i>include/GL</i>. Notice that 
<i>include/GL/gl_float.h</i> defines many inline functions converting
the most useful floating OpenGL calls to fixed point version.
</p>
<h2><a name="GPU">GPU Implementation</a></h2>
<p>
	This chapter gives more details about the GPU implementation.
</p>
<h3><a name="Comm">Communication between CPU and GPU</a></h3>
<p>
	The various commands sent to the GPU by the CPU (the communication from
GPU to CPU is limited to some flags and atomic integers for counting frames)
are small messages copied to a command buffer. We do not directly call GPU
functions from CPU programs, because we want two threads working in parallel 
(to match both GP2X design and today's taste for dual cores, SMP, etc). We do 
not neither use UNIX inter-process communication mechanisms (System V message 
queues, pipes, sockets...) because GP2X's 940 have no access to such 
mechanisms. So the only option left is to code a simple cyclic buffer, located 
in upper 32Mbytes and thus available from both CPU (mmapping /dev/mem from 
Linux, or direct access from the 940), large enough to holds thousands of 
commands. The buffer is located with various other data in the <i>shared</i>
structure at address <i>SHARED_PHYSICAL_ADDR</i>.
</p><p>
	We follow two objectives&nbsp;:
</p><ul>
		<li>The commands must be the smallest possible so that we can holds many
in the buffer and the 920 does not spare to much time writing them&nbsp;;</li>
		<li>The commands must be representable by simple C structures and must 
no be packed in order for the 940 to use them in place without the need to 
copy
the data out of the buffer&nbsp;;</li>
</ul><p>
	The chosen trade-off is as follow&nbsp;: we define a C structure for each 
command, composed only of 32 bits words (bit fields allowed for flags). Each 
of these structure starts with an opcode.
</p><p>
	Only the polygon command needs to be more polished, because a polygon can 
comes with three to many vertexes (although seldom more than 5), and with 
various parameters depending on the rendering mode (and we want the parameters 
to be in the same place on the command buffer, whatever the rendering mode, to 
be able to process all polygons the same until the draw-a-scanline function).  
Thus, the polygon function is split in two parts&nbsp;: a fixed part, the 
<i>gpuCmdFacet</i> structure which is 4 words long and tells how many vertexes 
are following and how to draw them, and then a variable part
consisting of as many <i>gpuCmdVector</i> as there are vertexes.
</p><p>
	This <i>gpuCmdVector</i> command which must always follow a 
<i>gpuCmdFacet</i> command (and thus has no opcode) holds 8 parameters (apart 
from the <i>same_as</i> member that we will see later when we look at 
projections). Those parameters appears in many shapes in a union, because 
depending on the rendering mode
the various parameters are located in different places. The important thing is 
that the first 3 parameters are the vertex coordinates, and then follow
some rendering parameters (how many depends on the rendering mode set in the 
<i>gpuCmdFacet</i>). So, the polygon function only have to know how many 
parameters there are, until drawing scan lines where those parameters are 
meaningful.
</p><p>
	Some troubles can arise when we met the end of the command buffer. A cyclic 
buffer is supposed to wrap at offset zero once the end is reached, but as we 
want the GPU to use the commands without copying them out of the buffer, we do 
not want a command to be split in half due to crossing buffer's end. So, when 
a whole command do not fit into what's left of the command buffer, we write a 
special command a single word length, opcode <i>gpuREWIND</i> telling the GPU 
to return to the beginning of buffer before reading next command. For the user 
application, the gpu940 library handle this wrapping automatically.
</p><p>
	The command buffer reading loop is in <i>bin/gpu940.c</i>, function 
<i>run()</i>. When no more command is waiting to be processed, the GP2X do a 
busy loop, while the PC yield the CPU to another process. On the GP2X, if the 
spinning longs too much (several seconds), the CPU speed is lowered to save 
batteries.
</p>
<h3><a name="Commands">GPU commands</a></h3>
<p>
	All command structures are defined <i>include/gpu940.h</i>. When some 
command is invalid, the GPU can set the <i>gpuEPARAM</i> error flag.
</p><p>
	<b>gpuREWIND</b> is a one word length command that inform the GPU that the 
next command is waiting at offset 0 of the command buffer. This is useful to 
avoid commands being cut by the end of the command buffer. The only effect is 
then to reset <i>cmds_begin</i> to 0. The user library handle this 
automatically.
</p><p>
	<b>gpuRESET</b> is used to reset GPU internal state (projection cache, JIT 
cache, buffers, window position, clip planes, perspective, frame counters, 
error flags, OSD state, performence counters, and all settable parameters).  
This should be used at the beginning of a program. The user library handle 
this automatically.
</p><p>
	<b>gpuSETVIEW</b> is used to configure the rendering window&nbsp;: its 
position, clipping borders, and perspective. The fourth first clip planes are 
updated to reflect the new clipping borders. The first cLip plane is the 
z-near clip plane, and is set to a very low value just to avoid division by 
zero during projection. OpenGl require this z-near clip plane to be 
user-defined, but for now it's constant.
</p><p>
	<b>gpuSETUSRCLIPPLANES</b> allow the user application to define additional 
clip planes (up to five user clip planes are supported, to a total of 10 clip 
planes).
</p><p>
	<b>gpuSETBUF</b> command allow the user to define the location of the 
buffers used in rendering. GPU uses up to three buffers, which types are 
defined in the <i>gpuBufferType</i> enumeration&nbsp;:
</p><ul>
	<li><i>gpuOutBuffer</i>, where the graphics are drawn,</li>
	<li><i>gpuTxtBuffer</i>, where the texels are fetched,</li>
	<li>and <i>gpuZBuffer</i>, where the Z values are read and writen for 
depth-tests.</li>
</ul><p>
	Each buffer is given a location (<i>struct buffer_loc</i>) made of it's 
offset from <i>shared-&gt;buffers</i>, its width (expressed in power of two) 
and its height. For texture buffer, height is required to be a power of two, 
but is not necessarily the same as width (although OpenGL enforces this).  
Width are not allowed to be greater than 18 (that is, 2<sup>18</sup> word 
values). Some parameters (a bit mask for texture size, and the address of 
rendering buffer) that are used internally by the generated rendering code 
depends on these values, so the JIT cache is flushed when this command is 
received.
</p><p>
	<b>gpuSHOWBUF</b> also gives a buffer position to GPU, but this buffer is 
not used for rendering. The address given with this command is used as the 
next buffer to display. More precisely, the buffer is queued on the 
<i>displist</i> list, which contains all the displayable buffers that were not 
already shown. The list act as a LIFO, so all buffers given are displayed 
sequencialy in the order received. Each time a vertical IRQ (or a periodic 
timer on PC) arise, a buffer is shifted of this list and displayed (configured 
on the MMSP display registers on the GP2X, slowly copied onto SDL buffers on 
PC), and the <i>frame_count</i> is incremented. If no buffers are available on 
the <i>displist</i> when the IRQ is raised, the <i>frame_miss</i> is 
incremented instead. The GPU never skips a published frame, so it's the 
responsibility of the application to use the frame counters to detect frame 
misses and react accordingly.
</p><p>
	<b>gpuPOINT</b> simply draws a pixel in a given color, at location given by 
a following <b>gpuCmdVector</b>. As this feature is merely used for debuging, 
no fancy rendering are available (like OpenGl width, antialias, or even light 
or textured (!) point).
</p><p>
	<b>gpuLINE</b> is nor implemented yet.
</p><p>
	<b>gpuFACET</b> is the more complex command, and the heart of GPU. It draw 
a (convex) polygon of any size (from 3 to 16 vertexes). It can do many 
different renderings&nbsp;:
</p><ul>
	<li>flat rendering,</li>
	<li>rendering with luminosity, with true or linear perspective,</li>
	<li>textured rendering, with true or linear perspective,</li>
	<li>textured rendering with luminosity, with true or linear 
perspective,</li>
	<li>keyed rendering (that is textured with a special color value meaning to 
skip output of the pixel), with or without luminosity, with true or linear 
perspective,</li>
	<li>smooth rendering (color interpolation) with true or linear 
perspective,</li>
</ul><p>
	all this with or without depth-tests, with optional blending. Also, two 
flags can mask the writing of pixel or depth values (all these &quot;with or 
without&quot; made a code generator mandatory). Also, a <i>cull_mode</i> 
parameter sets the culling mode for the facet. All this made this command 4 
words long, thanks to C bit fields. This command must be followed by as many 
<i>gpuCmdVector</i> as defined in the facet's <i>size</i>.
</p><p>
	Note that keyed rendering is an unknown feature for OpenGL. Instead, OpenGL 
can use an alpha component in the texture, which is far more expensive.  
Textures defined with an alpha channel are transformed by the OpenGL layer to 
keyed textures&nbsp;: when alpha channel is below an arbitrary limit (which is 
5/256), we replace the texel color by a key color are mark the texture as 
requiring a key test. Also, the GPU only supports constant blending, that is 
to use a constant blending ratio for all polygon, when OpenGL requires a 
dedicated component for alpha value. Anyway, using key color in conjunction 
with constant blending proved enough for Egoboo, wich is not that bad. Adding 
a dedicated alpha value could be done, though, and probably will in a near 
future&nbsp;: it will not cost much CPU cycles, and will considerably simplify 
OpenGL layer code.
</p><p>
	Not really a command by itself, the <i>gpuCmdVector</i> structure needs a 
proper explanation. The first field of this structure, <i>same_as</i>, can be 
used to skip the projection if the very same vertex position was used 
previously. If <i>same_as</i> is 1, it means that this vertex share the same 
position (and so, also the same 2D coordinates) than the just previously sent 
vertex. If it's 2, it means that this vertex share the same position than the 
vertex that was sent 2 vertexes ago, and so on. If <i>same_as</i> is 0, it 
means that this vertex is to be projected again. This value is automatically 
handled by the OpenGL library, and can save about one third of the required 
projections, with a very small cache of 8 positions.
</p><p>
	Next in the <i>gpuCmdVector</i> follow a union or parameters, the first 
three being the 3D coordinates, and next ones depending of the rendering mode 
set in the facet command. Notice that when depth test are enabled, or when the 
<i>write_z</i> flag is set in facet command, the Z coordinate compared to or 
written in the depth buffer is <i>not</i> the third 3D coordinate, but a 
separate parameter. This is so that it's possible to normalize depth 
parameters, as required by OpenGl (and common sense).
</p><p>
	<b>gpuRECT</b> command tells the GPU to fill a rectangle in one of its 
buffer. The rectangle position is given in pixels coordinates relative to 
buffer or to the clipping window. Notice that this is performed with regular 
<i>memset</i>, not using GP2X's fancy blitter.
</p><p>
	<b>gpuZMODE</b> sets the depth test mode. All OpenGL operators are 
available, plus the special <i>gpu_z_off</i> which disable depth test (and is 
the default).
</p><p>
	The last command is <b>gpuDBG</b>, which enable or disable (default) the 
OSD debugging console. The debugging console displays various performance 
counters, frame counters, GPU speed and error flags.
</p>
<h3><a name="PolyPersp">True perspective polygon drawing</a></h3>
<p>
	This chapter briefly explain how true-perspective mapping works, and how it 
is implemented in gpu940.
</p><p>
	When rendering 3D polygons, one should take care of rendering closer pixels 
&quot;bigger&quot; than farther ones. This is true for every rendering 
algorithm but the simpler one when all pixels share the same color, but the 
perspective question generally raise only when texture mapping is concerned, 
because for gouraud rendering, for example, the eye is very tolerant to 
approximations in this area. But when texture mapping enter the scene one must 
take perspective into account. Most of the time, to lower the amount of 
computation, one simply compute the true <i>(u,v)</i> coordinates once in a 
while and keep linearly interpolating these parameters between these computed 
values. This correction gives good results, but still require expensive 
divisions every 8 or 16 pixels. And ARM processors really hate divisions.
</p><p>
	The technique known as &quot;z-constant mapping&quot; works differently.
Instead of rendering scan-lines, which are distorted by perspective, one looks 
for the slope of constant depth on the polygon, and render along that slope.  
Mapping coordinates (as well as any other rendering parameters) can then be 
linearly interpolated without commiting any approximation.
</p><p>
	This technique is seldom used because it suffers from several drawbacks and 
misconceptions&nbsp;:
</p><ul>
	<li>It's hard to determine the z-constant &quot;scan-line&quot;&nbsp;;</li>
	<li>It's hard/impossible to perform advanced rendering tricks such as 
	subpixel rendering&nbsp;;</li>
	<li>It's hard to draw slopped &quot;scan-lines&quot;&nbsp;;</li>
	<li>It's hard to avoid empty gaps between congruent polygons&nbsp;;</li>
	<li>The cost of writting non-sequential pixels are overwhelming&nbsp;;</li>
	<li>etc ...</li>
</ul><p>
	Finding the slope of constant depth is not very difficult and can be 
achieved with only two divides. Drawing sloped lines is not very complex 
because it basically consists of adding a constant slopping factor to every 
pixel's X or Y coordinate, factor which is proportional to this coordinate.  
Apart from that (finding this slope parameter and adding it to the 2D 
coordinate of each pixels), there are no more difference between this true 
perspective polygon routine and a polygon routine unaware of perspective.  
That's why it's not impossible at all to perform sub-pixel accuracy (notice 
that there are no sub-pixel accuracy rendering in gpu940).
</p><p>
	Filling gaps between polygons, at the contrary, is a real difficulty, but 
can be solved. Also, writing non sequential pixels can have terrible 
consequences for performance. But not for GP2X's ARM940, because&nbsp;:
</p><ul>
	<li>each pixel is 32 bits in YUV format&nbsp;;</li>
	<li>data cache is not used for writes in the out-buffer&nbsp;;</li>
</ul><p>
	In theory, then, the GP2X was a perfect fit for this technique. Anyway, it 
was later discovered than, even if data cache is unused, writing words far 
away from each other is much slower than writing words that are close from 
each other, so the true-perspective mapping must be reserved for when it's 
really needed, that is for big polygons that covers a large portion of the 
screen. That's why the true-perspective functionality can be switched off on a 
polygon by polygon basis. Hopefully, because z-constant routine is very close 
to no-perspective routine, its almost the same in both cases&nbsp;: enabling 
true perspective just adds the slope computation and the addition of the 
sloping
factor to one of the 2D coordinates of every pixels.
</p><p>
	The true perspective polygon &quot;shaper&quot; can be found in 
<i>bin/poly.c</i>, while its no-perspective counterpart lie in
<i>bin/poly_nopersp.c</i>. Currently, the OpenGL layer does never enable
true perspective rendering. Its result can only be seen in the
<i>code alone</i> demo or other sample programs under <i>sample</i>
directory.
</p>
<h3><a name="PolyNoPersp">No perspective polygon drawing</a></h3>
<p>
	The fact that there exists two distinct source files for perspective and
no-perspective rendering is just a matter of performance&nbsp;: the few 
avoided <i>if</i>s made a significant speed improvement&nbsp;; but both files are 
still
mostly identical. Also, the routines that render the pixels along a scan-line
(sloped or not) are exactly the same. It surely is problematic to have such
similar code being duplicated, and this is one of the objectives for future 
release to have both polygon &quot;shaper&quot; merged into one back again.
</p><p>
	Anyway, the fact that we do not always draw sequential pixels must be taken 
into account in the &quot;Just In Time&quot; (JIT) code generator.  As we will 
see shortly, when true perspective is disabled the JIT code generator can 
sometime exploit an ARM-related trick to build faster code.
</p>
<h3><a name="YUV">Handling YUV color model</a></h3>
<p>
	Because it was found that having 32 bits per pixels leads to better 
graphics and simpler code, we use YUV mode on the GP2X. This is supposed to be 
transparent for the user, though, so that the PC version can still use 
customary RGB colors and so that OpenGL color model can be easily handled.  
Thus, each time a color is given to the GPU, its supposed to be filtered by a 
host to GPU converter, which convert from RGB to YUV on GP2X and which does 
nothing on PC. Similarly, all textures given to the helper library (atop of 
which the OpenGL layer is build) is converted from RGB color to GPU color 
model.
</p><p>
	Many inline functions are defined in <i>include/gpu940.h</i> that perform 
this conversion. Often, inside gpu940's code, color components are named 
<i>r</i>, <i>g</i> and <i>b</i>. These names really should be 
<i>color_component_0</i>, <i>color_component_1</i> and 
<i>color_component_2</i> for they are really just that, and may be YUV instead 
of RGB.
</p><p>
	Only in the lighting code does the distinction between YUV and RGB matters.  
In YUV mode, lighting is done by modification of the Y (luminosity) component 
alone, while in RGB color model lighting imply modification of the three 
components.
</p>
<h3><a name="JIT">"Just In Time" code generation</a></h3>
<p>
	To face the exponentially growing number of rendering inner loops since the 
addition of an OpenGL layer, a code generator was added to gpu940. To call 
this code generator a &quot;Just In Time&quot; <i>compiler</i> is certainly 
presumptuous, but they call it like this in the <a 
href="http://ogl-es.sourceforge.net/">Vincent GL-ES</a> library, where I 
borrowed the name from.
</p><p>
	What the code generator does is simply to assemble code fragments that does 
simple things like reading a texel, adding linear coefficient to parameters, 
testing depth, writing pixel, etc, in a way that the resulting code block 
performs the required rendering of a (optionally sloppy) scan line.
</p><p>
	First, have a look at the <i>raster_gen()</i> function (here, <i>_gen</i> 
is for &quot;generic&quot;) in <i>bin/raster.c</i>. This function is used when 
the JIT is not used (for the PC version). This can draw a scan line in any of 
the available rendering model. Of course, it's crowded with tests and 
conditional branching, so has very poor performance. But it gives the skeleton 
of what the correct routine must do. It's counter part with JIT is
the <i>bloc_def_func()</i> function in <i>bin/codegen.c</i>. This later 
function have the same flow than <i>raster_gen()</i>, but instead of 
performing the operations needed to render the scan line it merely calls a 
user supplied function with a tag identifying the spot reached in the flow and 
the action that's supposed to be carried there. Instead of performing this 
operation, the call back job is to generate the code that will performs
it. So that when <i>bloc_def_func()</i> exit we have a minimal, branch free
routine that draws a scan line in the current context's rendering mode.
</p><p>
	Doing this efficiently is not that simple, though. First, you have to list 
all needed parameters, and affect them to registers (hopefully, ARM processors 
comes with many of those). It's not possible to have a fixed mapping here 
because we can need many many different parameters. Those registers will be 
loaded with parameters value at the beginning of the function, and will serve 
no other purpose in the inner loop. So you have to know this mapping between 
parameters and registers before starting to output code (actually, the first 
code to be output is this loading of parameters values into those registers).  
Also, you have to know how many registers will be needed for temporary 
computations. For example, you may need two temporary registers to compute 
texel address, then later three temporary registers to compute a blending 
effect, so that you need, in total, three temporary registers (if you though 
five I caught you - they are <u>temporary</u> registers, and can be reused).
</p><p>
	This is why the <i>bloc_def_func()</i> function takes a callback function 
as parameter&nbsp;: it's called several times to generate a code block&nbsp;: 
the first time to allocate registers, the second time to actually write the 
code.
</p><p>
	In fact, it can be called a third time, for what I call the <i>bottom 
half</i> of the code block (I lacked a name for this, so I used the BSD kernel 
&quot;bottom half&quot; designation, meaning &quot;another piece of code that 
needs to be executed after, for which no better name came to mind&quot;).
</p><p>
	To understand what JIT's &quot;bottom halves&quot; are good for, you need 
to remember a previous remark&nbsp;: ARM processors know a clocking trick to 
access RAM faster when they know in advance that the accessed addresses will 
be sequential. In other words, the <i>stm/ldm</i> instructions can write and 
read memory faster than several distinct otherwise equivalent <i>str/ldr</i> 
instructions (this has to do with RAM bank selection IIRC). So, when we know 
that we are going to write sequential values, it pays to prepare several 
pixels first and then write them to the out buffer with a single <i>stm</i>.  
So, the register allocator try to compute how much pixels we can hold in 
remaining registers and write in a single burst, then we have to handle this 
case in all functions that output the code. But this is not enough&nbsp;: when 
the scan line length is not a multiple of this number of pixels, we have to 
rely on the simpler, pixel by pixel version of the code to end the scan line.  
This is what's called the bottom half in the JIT compiler.
</p><p>
	So, in the general case, the <i>bloc_def_func()</i> function is called 
three times&nbsp;: one to allocate registers, one to output the code that draw 
pixels in &quot;burst mode&quot;, and another time to output the code that 
completes the scan line one pixel at a time.
</p><p>
	Of course, all this machinery cannot be called each time we need to draw a 
scan-line (not even each time we need to draw a polygon). That's why the JIT 
compiler keeps up to 5 generated routines, each identified by a key build from 
the rendering context (a combination of the various parameters that are used 
to build the drawing routine). New needed routines take the place of the last 
recently used ones when necessary. Also, to keep the JIT cache reasonably 
small, each routine is limited to about 100 instructions (which proved enough 
empirically).
</p><p>
	Notice&nbsp;: on PC, although the JIT code is not executed, it's still 
generated and written to files in <i>/tmp/codegen_*</i>, so that one can have 
a look at the resulting code. <i>make  dumpcodegen</i> in <i>gpu940/</i> 
directory disassemble all the code to standard output.
</p>
<h2><a name="libgpu">Helper library</a></h2>
<p>
	This chapter gives details about the gpu940 library, build in <i>lib/</i>, 
and whose API is defined at the end of <i>include/gpu940.h</i>.  
<i>libgpu940.so</i> is the user interface the the GPU shared memory, and 
provides many useful functions.
</p>
<h3><a name="need4lib">The need for a client library</a></h3>
<p>
	Gpu940 comes with a little library that helps clients to send commands and, 
more importantly, to manage the video buffer. As the client must provides 
every buffers location by address, it's up to the client to allocate space for 
textures, depth buffer, output buffers, and to free them when they are not 
used anymore - which can be tricky for the output buffer, since it's used for 
rendering <u>and</u> for displaying, and thus must not be recycled until the 
<i>next</i> frame is displayed (not only published with <b>gpuSHOWBUF</b> but 
actually displayed on the screen).
</p><p>
	Also, the <b>gpuREWIND</b> command add complexity to the command buffer, 
and this library provide a simple API for sending commands that is similar to 
the familiar <i>write()</i> and <i>writev()</i> UNIX functions.
</p><p>
	So you are encouraged to use this library even if you plan to not use the 
OpenGL layer for performance reason. OpenGL library uses it. Even the <i>Code 
Alone</i> demo uses it.
</p>
<h3><a name="initialization">Initialization</a></h3>
<p>
	The <i>gpuOpen()</i> function, located in <i>lib/gpu940.c</i>, needs to be 
called before anything else to ensure proper communication with GPU&nbsp;: it 
mmaps <i>/dev/mem</i>, sends a <b>gpuRESET</b> command to the GPU then waits 
until the frame count in <i>struct shared</i> is zero. This is important 
because further memory allocation functions can make use of this frame 
counter.
</p><p>
	What this initialization function does not perform, though, is loading the 
gpu940 code into the ARM940, nor even ensuring that the gpu940 software is 
correctly loaded onto the 940 (or that the x86 gpu940 binary is running).  
There is currently no way for the client program to find out if the GPU is 
loaded or not.
</p><p>
	<i>gpuClose()</i> function should be called at the end of the client 
program to close the mmaped file. Notice that it does not &quot;unload&quot; 
(what would that mean&nbsp;?) gpu940 from the ARM940 memory, nor stop it in 
any way. The GPU will enter standby mode by itself (ie. reducing it's clock 
speed as low as 50MHz). To stops the ARM940 from executing some code, the 
stop940 program must be run (this apparently does not work, see <a 
href="https://gna.org/bugs/index.php?8404">bug description</a>).
</p>
<h3><a name="commandbuffer">Handling the command buffer</a></h3>
<p>
	To send commands to the GPU, the helper library offer two functions that 
are the equivalent of the UNIX <i>write</i> and <i>writev</i>&nbsp;:
</p><ul>
	<li><i>gpuWrite();</i></li>
	<li><i>gpuWritev();</i></li>
</ul><p>
	Sent commands must be word aligned, and all sizes are given in bytes 
despite they are constrained to 4 bytes multiples. These functions handle 
<b>gpuREWIND</b> properly, and guaranty that all writes are atomic (ie. the 
GPU will see all or none).
</p><p>
	Also, these functions takes a boolean <i>can_wait</i> parameter, which, 
when true, allow the library to block until enough space is available in the 
command buffer. When the library must wait, is busy loop over a call to the 
<i>sched_yield()</i> function, which will yield CPU to another ready to run 
task. If no such other task is running, this is equivalent to a busy loop, 
which is not friendly for GP2X batteries. A client application should be aware 
that it's waiting for the GPU, and lower it's CPU clock accordingly.
</p>
<h3><a name="videobuffer">Handling the video buffer</a></h3>
<p>
	All video buffer handling takes place in <i>lib/mm.c</i>. The simplest 
functions are <i>gpuAlloc()</i> and <i>gpuFree()</i> which are equivalent to 
the standard libc <i>malloc()</i> and <i>free()</i> except that they do not 
operate on <i>void *</i> pointers, but on a <i>struct gpuBuf</i> structure, 
which describe more precisely the video buffer segments that are allocated.  
All these <i>gpuBuf</i> objects are chained together in two lists&nbsp;:
</p><ul>
	<li><i>list</i>&nbsp;: which is the list of all allocated <i>gpuBuf</i> in 
increasing addresses order (addresses in video buffer)&nbsp;;</li>
	<li><i>fc_list</i>&nbsp;: which is list of all buffers that will 
automatically be freed when the frame count will reach a given value (more on 
that below)&nbsp;;</li>
</ul><p>
	Objects of that type are created on a dynamically allocated cache 
(<i>buf_cache</i>). They are kept distinct from the video buffer because as 
the list is often sequentially scanned by the allocator, it's better to have 
it in the ARM920 data cache (plus, it lowers the amount of needed pointer 
arithmetic). The <i>cache_list</i> list chains together all free 
<i>buf_cache</i> objects.
</p><p>
	The opaque <i>struct gpuBuf</i> type holds the <i>struct buffer_loc</i> 
type that is needed by the buffer manipulation GPU commands. The 
<i>buffer_loc</i> value can be obtained from a <i>gpuBuf</i> object with the 
<i>gpuBuf_get_loc()</i> function.
</p><p>
	<i>gpuAlloc()</i> takes its sizes with two parameters&nbsp;: 
<i>width_log</i> and <i>height</i>. So, the width is constrained to a power of 
two, as all buffers given to the GPU share this constraint.
</p><p>
	More strangely, it also takes a boolean parameter named <i>can_wait</i>.  
This is because in some circumstances the library can free some allocated 
buffers from within <i>gpuAlloc()</i>&nbsp;: when those buffers were 
&quot;freed&quot; with the special <i>gpuFreeFC()</i> function. This one does 
not free the buffer at once, but rather takes a number of frames and add the 
buffer onto the <i>fc_list</i> for later removal when that number of frame 
counts will be ellapsed. This strange semantic has two purposes.
</p><p>
	First, it allow the client application to say something like &quot;free 
this buffer when the current frame is done&quot; (with <i>gpuFreeFC(buffer, 
1)</i>). To understand why this is required, imagine the following program, 
that wants to&nbsp;:
</p><ol>
	<li>allocate a buffer B1, then load a texture in it&nbsp;;</li>
	<li>draw some polygons using texture B1&nbsp;;</li>
	<li>load another texture into B1&nbsp;;</li>
	<li>draw some other polygons that use this other texture&nbsp;;</li>
	<li>publish the frame&nbsp;;</li>
	<li>free buffer B1&nbsp;;</li>
</ol><p>
	This buggy application may end up with all polygons mapped with the second 
texture, because nothing prevents the GPU to start drawing the polygons only 
once the client program published the frame, or worse freed it (if it was 
still rendering previous frame for example)&nbsp;! Don't forget that there are 
<u>no</u> synchronisation between CPU and GPU, even at the point when the CPU 
send a <b>gpuSHOWBUF</b> (&quot;publish&quot; the frame). The above program 
can only work if it uses two distinct buffers for the two textures, and if it 
ask the helper library to wait until next frame was displayed before freeing 
the buffers. So, most of the time, you will use <i>gpuFreeFC()</i> instead of 
<i>gpuFree()</i>.
</p><p>
	The other reason why this delayed free is useful is to allow the user 
program to work several frames in the future&nbsp;: when a frame is published, 
all the buffers required to build this frame can be freed at once, some other 
may be useful only until next frame (eg. the display buffer, which must not be 
reused before another frame is displayed), and in some cases (blur motion 
effects&nbsp;...) some buffers may be needed longer.
</p><p>
	This feature can only work if the helper library know when the user publish 
a frame. That's why a special <i>gpuShowBuf()</i> function is there.  Don't 
ever use <i>gpuWrite()</i> to send a <b>gpuSHOWBUF</b> command, or the library 
internal frame count would be wrong.
</p><p>
	For convenience, a <i>gpuSetBuf()</i> function is available that sends a 
<b>gpuSETBUF</b> command corresponding to a <i>gpuBuf</i> object.
</p>
<h3><a name="synchro">Synchronization with GPU</a></h3>
<p>
	The same library source file hosts the <i>gpuWaitDisplay()</i> function.  
It simply waits until the GPU frame counter (available in <i>shared</i>) gets 
equal to or greater than the internal frame counter (minus 1). That is, it 
waits until the second last published frame's address is set in the display 
controller (where <i>shared->frame_count</i> is incremented).
</p><p>
	Why not waiting until the last published frame is displayed&nbsp;? Because 
then, if the vertical IRQ occurs faster than the GPU do render frames, the GPU 
is hold idle between the time when it add the last frame to the display list 
and the time next IRQ occurs and displays it&nbsp;; and having the GPU idle when 
it's late is not a good idea.
</p>
<h2><a name="OpenGL">OpenGL Implementation</a></h2>
<p>
	This chapter will try to help you understand how (some of) the OpenGL API 
calls were mapped onto the GPU commands. You are probably already familiar 
with legacy <a href="www.opengl.org">OpenGL</a>.
</p>
<h3><a name="GLMachine">Overall OpenGL machine</a></h3>
<p>
	At first, OpenGL appears as a client-server state machine. For our purpose, 
there are no need to distinguish between client and server states.  No such 
slient-server design is planned, although it would be possible in theory.
</p><p>
	The OpenGL library was implemented in a straightforward manner. As a 
result, most of the source code in <i>GL/</i> directory only maintain the 
state variables without consideration for the actual renderer. Sometimes, this 
can gives the impression that OpenGL features are implemented while they in 
fact produce no visible result. Thus, most o the code in the GL library will 
not be discussed here.  </p><p>
	The mapping between the GL state and the renderer is done mainly in 
<i>GL/cmd.c</i>. Each time a vector is submitted to the GL in a way or 
another, the <i>gli_cmd_vertex()</i> function is called, which send a 
<b>gpuFACET</b> command when necessary. This function is the narrow bridge 
between an idealized GL state machine and the GPU, and will be detailed 
hereafter.
</p>
<h3><a name="GL2GPU">Binding to the GPU</a></h3>
<h4><a name="GLcoord">Vertex coordinates</a></h4>
<p>
	The <i>gli_cmd_vertex()</i> function, which is only given a pointer to the 
fixed point complete vertex coordinate set (4 components&nbsp;: x<sub>0</sub>, 
y<sub>0</sub>, z<sub>0</sub> and w<sub>0</sub>), first computes the required 
coordinate sets. Here are the coordinates used in the transformation pipeline 
of the GL library, in order of appearance (refer to your OpenGL 
specifications, "Coordinate Transformations")&nbsp;:
</p><ol>
	<li>
		<u>eye coordinates</u> (x<sub>e</sub>, y<sub>e</sub>, z<sub>e</sub>, 
w<sub>e</sub>). They are the incoming vertex coordinates transformed by the 
<i>modelview</i> matrix, ie. the vertex coordinates in the camera space.  
These coordinates are further needed to compute colors when lighting is on.
	</li><li>
		<u>clip coordinates</u> (x<sub>c</sub>, y<sub>c</sub>, z<sub>c</sub>, 
w<sub>c</sub>). They are computed from the eye coordinates transformed by the 
projection matrix, which &quot;scale&quot; this position according to desired 
frustum. These are <i>almost</i> the 3D coordinates that will be sent to the 
GPU. Notice that most of the time w<sub>c</sub> is simply -z<sub>e</sub>.
	</li><li>
		<u>normalized device coordinates</u> (x<sub>d</sub>, y<sub>d</sub>, 
z<sub>d</sub>). These are supposed to be the 3D clip coordinates divided by 
w<sub>c</sub> (perspective division). This leads to coordinates ranging from 
-1 to 1, thus the name. We do not compute these, because we want this 
perspective division to be performed by the GPU (remember&nbsp;?).
	</li><li>
		<u>window coordinates</u> (x<sub>w</sub>, y<sub>w</sub>, z<sub>w</sub>).  
These are simply the previous coordinates scaled to the window size (width and 
height) and desired depth range. This is done by the GPU, which by the way 
uses it's own set of parameters to define the window position (compared to GL 
window coordinates, GPU window coordinates have Y toward the bottom, and 
accordingly Z increasing as depths increase).</li>
</ol><p>
	A little maths is needed here to split these computations between the GL 
library and the GPU (where we want the divisions to take place). First, we are 
supposed to divide by w<sub>c</sub> despite the GPU actually divides by the Z 
vertex coordinate&nbsp;: so we give w<sub>c</sub> for Z. Before doing this 
division, the GPU multiplies the X and Y coordinates by it's <i>dproj</i> 
(defaulting to 256). Thus we must divide the given X and Y coordinates by this 
<i>dproj</i>.  Finally, the GPU does not scale vertex coordinates to match 
window size, so we must do this in the GL library, by multiplying clip 
coordinates by the viewport width and height. The vertex projected position 
will then be located according to GL specifications.
</p><p>
	The only case left is z<sub>w</sub>. We already saw that the GPU performs 
depth testing with a dedicated parameter instead of the Z vertex coordinate, 
which comes in handy&nbsp;: this corresponds to GL's z<sub>w</sub>. Notice how 
the two Zs have slighty different meanings&nbsp;: one is a perspective divisor 
while the other is a depth. z<sub>w</sub> is supposed to be equal to 
(z<sub>c</sub>&nbsp;/&nbsp;w<sub>c</sub>)&nbsp;&times;&nbsp;((f-n)&nbsp;/&nbsp;2)&nbsp;+&nbsp;((f+n)&nbsp;/&nbsp;2).
This expensive computation is performed only if depth parameter is required 
(depending mainly on the <i>GL_DEPTH_TEST</i> state).
</p><p>
	Now all our vertex coordinates are properly set.
</p>
<h4><a name="GLcolor">Pixel colors</a></h4>
<p>
	Here is schematically where pixels colors came from with OpenGL&nbsp;: 
first, each vertex is given a color (from the current color given with 
<i>glColor()</i> or a material color). These colors are modified by lights if 
lighing is enabled. These resulting colors (3 R,B,G components and an 
additional alpha component per vertex) are then interpolated through the 
polygon. Then, if texturing is enabled, a texel if fetched and the vertex 
color and texel color are mixed according to some environment parameter (by 
default, they are multiplied). This gives the final color, which is then 
written straight to the frame buffer or mixed with the previously stored color 
if blending is enabled.
</p><p>
	This leads to potentially mixing colors two times per pixel, that is 4 
multiplications per pixel and per color component&nbsp;: 12 multiplications 
per pixel if implemented straightforwardly.
</p><p>
	Of course we can not afford this, so several tricks are used.
</p><p>
	First, like already state, most of the time the vertex colors are the same 
for all vertexes of a polygon, and lighting does only affect the luminosity 
(when using white light). So we do not need to interpolate 4 coefficients but 
only one, the luminosity (which can be merely written in the frame buffer in 
YUV mode). gpu940's libGL try to recognize these cases with the 
<i>gli_simple_lighting()</i> function. When it seams possible, the GPU flat 
shading is used with a custom luminosity parameter. Otherwise it uses the full 
R,G,B mode, when all three color components are interpolated.
</p><p>
	Then comes texturing. Most of the time when texturing is on, vertex colors 
are not used or are used only to light the texture. Then again it is 
sufficient to write the texel with modified Y component. Actually gpu940 GL 
library does not try to recognize when mixing texel and vertex colors is 
necessary, and always assume no lighting (which is why Egoboo2x lighting 
effects appears to be disabled). In the future, the library should try to use 
the luminosity parameter to render lighted textures. Full color and texel 
mixing seams out of reach for now.
</p><p>
	Then comes blending. Blending is mainly used for two purposes&nbsp;:
</p><ul>
	<li>render translucent primitives&nbsp;;</li>
	<li>achieve texture transparency (some part of the texture are fully 
transparent while the remaining texels are fully opaque)&nbsp;;</li>
</ul><p>
	For both, it's not necessary to store resulting alpha values in the frame 
buffer (actually, even some hardware implementations do not store alpha 
components, and the OpenGL standard seams to allow this).
</p><p>
	The eye is very tolerant with translucency&nbsp;: in fact, one can hardly 
tell the difference between two levels of blend if they are close together.
So that it's enough to have only, say, 8 possibles blending levels, and that 
this blending level can be constant through a whole polygon. Gpu940 avoids the 
required multiplication by using a color mixing routine based on a constant 
amount of shifts. The <i>blend_coef</i> polygon parameter choose the amount of 
shift between the old and new color components&nbsp;: the previous components 
are shifted in one direction and the new components in the other direction, 
then both a summed and the result written to the frame buffer. The libGL tries 
to map the GL mean alpha value to this <i>blend_coef</i> parameter.
</p><p>
	When alpha value is supposed to be fetched from the texture, this is 
another story. When a texture with an alpha component is uploaded, all texels 
alpha values are examined and if the mean alpha value is below a threshold, 
it's assumed the user want translucency (as describer above). In addition, if 
some alpha values are very low, it's then also assumed that the user want to 
achieve transparency. Gpu940 have a distinct parameter for this&nbsp;: texture 
key color. When a texture with an alpha component is uploaded, all texels that 
have their alpha component below a threshold are replaced with the key color, 
and the texture is tagged with a flag (<i>need_key</i>) that will lead to the 
GPU using the key-color rendering mode.
</p><p>
	Both transparency and translucency are then feasible without the need to 
multiply color components with alpha components, at the expense of additional 
guess-work in the library side (and some visual imperfections).
</p>
<h3><a name="GLswap">Swapping buffers</a></h3>
<p>
	OpenGL does know about only one frame buffer, where all the primitives are 
rendered. This is the job of the GLX X-Windows extension (or the eglx library 
for OpenGL-ES) to take care of what's actually displayed. GLX (or its eglx 
counterpart) frame abstraction is very much simpler than gpu940 one, because 
it knows of only two frame buffers&nbsp;: the front one (displayed) and the 
back one (where primitives are renderer). When the application want to display 
a frame, it only have to call the <i>glXSwapBuffers()</i> function to have the 
back buffer become the front one.
</p><p>
	To offer the same behavior to applications, gpu940 GL library uses three 
sets of buffers, each set consisting of an out buffer and an optional depth 
buffer (only allocated if <i>glOpen()</i> was called with the 
<i>DEPTH_BUFFER</i> flag). It then provides the <i>glSwapBuffers()</i> 
function which mimic the behavior of the equivalent GLX function.
</p><p>
	Why using three buffers instead of merely two&nbsp;? Remember the 
discussion about <i>gpuWaitDisplay()</i>&nbsp;: this function waits only until 
the second last published frame gets displayed, for performance reason. So we 
cannot reuse this one, since it's currently displayed, nor obviously the one 
we just published, for it's going to be displayed. We must then work with 
three sets of buffers.
</p>
<h3><a name="GLfloats">Other types than GLfixed</a></h3>
<p>
	Although the softfloat library used for floating point arithmetic is very 
optimized, it cannot cope with well optimized fixed point arithmetic on a 
processor that have no hardware support for floats. All GPU internal as well 
as the helper and GL libraries are thus fixed point only.
</p><p>
	Most of OpenGL applications, though, insist on using floats as their main 
data type for geometry. For simplicity, a quick and dirty header file was 
coined to convert some popular floating point version of OpenGL calls into 
their fixed point counterparts. This header file is 
<i>include/GL/gl_float.h</i>, and is included by <i>include/GL/gl.h</i>. It 
uses inline C functions rather than preprocessor macros for better type 
checking. Apart from that, these functions are not optimized in any way 
(matrix conversions are particularly awfull). This header file is provided to 
make porting existing applications easier. If you can, avoid using them and 
keep away from floats altogether.
</p>
<h2><a name="App">Appendix</a></h2>
<p>
	The following notes are some advices on how to use gpu940 to write or port 
OpenGl applications. If you followed everything until there it's likely that 
everything will just sound like common sense to you.
</p>
<h3><a name="AppBrew">How to write new OpenGL apps</a></h3>
<p>
	You should be aware of two things :
</p><ul>
	<li>gpu940's GL library is <u>not</u> compliant to the OpenGL standard. 
Although its designed to help porting applications in and out, we can not 
implement the OpenGL specs but a small subset of it&nbsp;; and even this subset 
is not implemented according to the specs&nbsp;;</li>
	<li>gpu940 &quot;GPU&quot; is about several hundred times slower than the 
hardware GPU that comes with your PC&nbsp;;</li>
</ul>
<h4><a name="ABConf">Conformance issues</a></h4>
<p>
	Here is a quick list of the missing features, compared to OpenGL 
specifications v1.1 :
</p><ul>
	<li><b>Client-server :</b> we do not distinguish between client state nor 
server state. As a consequence, to GL apps can not share the GPU.</li>
	<li><b>Argument types :</b> we do only support <i>GLfixed</i> type, which 
is a creation of OpenGL-ES. Some of the other types are supported via quick 
and dirty wrappers.</li>
	<li><b>Evaluators :</b> are not implemented.</li>
	<li><b>GL_LINES, GL_LINE_STRIP, GL_LINE_LOOP :</b> are not implemented, but 
should be.</li>
	<li><b>GL_POLYGON :</b> are not implement but will certainly be before 
long.</li>
	<li><b>Polygon edges :</b> are not implemented.</li>
	<li><b>Indexed colors :</b> are not implemented.</li>
	<li><b>glInterleavedArrays() and glRect() :</b> are not implemented, but 
should be.</li>
	<li><b>Normalization :</b> is not implemented. Could be, but probably not 
a good idea.</li>
	<li><b>Texture coordinate generation :</b> is not implemented.</li>
	<li><b>Clip Planes :</b> not implemented yet, but should be. Anyway, gpu940 
allows only 5, GL specs requires at least 6.</li>
	<li><b>All raster functions :</b> are not implemented.</li>
	<li><b>SpotLights :</b> are not implemented, but should be.</li>
	<li><b>Color material :</b> is not implemented, but should be.</li>
	<li><b>Antialiasing :</b> is not implemented. Low on priority list.</li>
	<li><b>Stippling :</b> is not implemented.</li>
	<li><b>Depth offset :</b> is not implemented, but could be.</li>
	<li><b>Pixel operations :</b> <i>glDraw/Read/CopyPixels()</i> are not 
implemented, nor <i>glBitmap()</i>.</li>
	<li><b>Proxy textures :</b> are not implmented.</li>
	<li><b>Texture matrix stack :</b> is unused.</li>
	<li><b>Texture parameters :</b> most have no effect.</li>
	<li><b>mipmapping :</b> is not implemented, but should be.</li>
	<li><b>1D textures :</b> is not implemented.</li> 
	<li><b>Texture prioritization :</b> is not implemented.</li>
	<li><b>Fog :</b> is not implemented but could be tricked.</li>
	<li><b>Scissor test :</b> not implemented, but should be.</li>
	<li><b>Alpha test :</b> not implemented, but should be.</li>
	<li><b>Stencil buffer :</b> there are none.</li>
	<li><b>Dithering :</b> is not implemented.</li>
	<li><b>Logic operation :</b> are not implemented. Could be if needed.</li>
	<li><b>Buffer selection :</b> is not implemented. Could be if needed.</li>
	<li><b>Accumulation buffer :</b> is not implemented.</li>
	<li><b>Evaluators, Selection, FeedBack, display lists :</b> are not implemented.</li>
</ul><p>
	And here is a quick list of what, while implemented, does not conform to 
requirements :
</p><ul>
	<li><b>Floating points :</b> we do all arithmetic on fixed point 32 bits 
integers, with 16 bits for the decimal part (GLfixed as defined by OpenGL-ES).  
Our range of acceptable values is then not conforming.</li>
	<li><b>GL_POINTS :</b> just a single pixel in size. Enough for debugging, 
which is the main purpose for points.</li>
	<li><b>Lighting simplification :</b> sometimes (refer to the above 
explanations to know how) libGL choose to interpolate only a single 
luminosity color component (useful in YUV mode) rather than the three R,G,B 
color components. This can leads to incorrect colors.</li>
	<li><b>FRONT_AND_BACK only material :</b> conforming to OpenGL-ES, its not 
possible to have different material properties for front and back faces.</li>
	<li><b>Polygon mode :</b> only <i>GL_FILL</i> is implemented.</li>
	<li><b>Texture environment :</b> other than <i>GL_REPLACE</i> may not work 
as expected.</li>
	<li><b>Blending :</b> DST_COLOR is unusable since we have no alpha buffer. 
Also, blending results are not compliant because we do not scale color 
components but just shift/add them.</li>
	<li><b>Color mask :</b> it's not possible to disable individual 
components. This is all the color or nothing.</li>
</ul>
<h4><a name="ABPerf">Performance issues</a></h4>
<p>
	First of all, you should be aware that if you do not pay extra attention to 
your rendering loop, the performance will be very deceiving. With the growing 
availability of very fast hardware accelerated 3D renderer, most of people 
that happen to develop new 3d applications tend to think that drawing a 
polygon is free, when actually even the simplest <i>glClear()</i> call is not 
free&nbsp;: it requires the GPU to write several thousands of  words onto 
uncached memory. So most of younger developers that are not familiar with 
software renderers will have to learn what are the consequences of the library 
functions they use. This is a good training anyway, since even when it's for 
the latest high-end 3D graphic card it never helps to waste power.
</p><p>
	So lets see how one can optimize an OpenGL program, especially when it's 
for gpu940.
</p>
<h5>Don't clear too much</h5>
<p>
	Like said in introduction, clearing the buffers can be time consuming and 
is not always useful. First, you may need to clear only the color buffer or 
the depth buffer.
</p><p>
	Then, always ask yourself what proportion of the frame will show the clear 
color once the whole picture will be rendered&nbsp;? Most of the time, not 
much, if at all&nbsp;; so that it's often better to add some clearing polygons (for 
example a sky box) than to clear the whole screen and redraw it completely.
</p>
<h5>Don't abuse depth tests</h5>
<p>
	Z-buffer was an evil invention. It's required for some pathological cases, 
when sorting polygons cannot, locally, give correct results. Most of the time 
the faster painter algorithm just works, so use it. Also, you often have 
something like a background with object in front of it&nbsp;: no need for any 
sorting nor depth testing for rendering these cases properly (in Egoboo2x for 
example, all walls and characters are always in front of all floors). It also 
helps to fight rendering inaccuracies that leads to make a character foot 
appears as if it is cut by the floor if its Z coordinate is a little too far.  
It also avoid to need the ugly z-offset when rendering wall-marks, etc...
</p><p>
	Using Z-depth does not imply you need resulting depths. For example when 
you render an object with depth tests enabled, you may not need this object 
depth values if you do not render another object against this one. In these 
situations, masking out z writing (with <i>glDepthMask()</i>) allow the GPU to 
skip writing a word per pixel.
</p><p>
	The same is true the other way round&nbsp;: you may need an object depth 
for later depth filtering without the need to test this object depth again the 
current z buffer. You can unmask depth writing and disable depth testing 
altogether.
</p>
<h5>Don't send useless polygons</h5>
<p>
	The GPU performs clipping in order to draw only visible portions of 
polygons. Not to dispense you from the need to implement a proper visibility 
filter or occlusion culling algorithm. Know your frustum and skip yourself 
trivial polygons. Implement occlusion portals, or other techniques specific to 
your case. You know your program and its particularities, the GPU does not.
</p><p>
	Also, when rendering closed surfaces, don't send back face polygons. If you 
have no special way to know that a polygon is back face (like a BSP), let the 
libGL do this for you by enabling <i>GL_CULL_FACE</i>.
</p>
<h5>Use simple meshes</h5>
<p>
	Low poly count is an art, and some of your users will enjoy it. Don't hide 
your geometry behind hundreds of smooth polygons. Low polygon models are nice, 
generally more easy to animate well, and require less space.
</p><p>
	That said, use several level of details or your meshes if your application 
will render them at different scales.
</p>
<h5>Use simple textures</h5>
<p>
	Low pixel count is an art, and some of your users will enjoy it (OK, maybe 
not that many, but I will :-)). Textures can be accessed much faster when they 
are small. Experimentally, it was found that on gpu940 it is noticeably faster 
to use a texture of 64&times;64 pixels than one of 128&times;128 pixels.
</p><p>
	That said, OpenGL provide mipmapping which have the double advantage of 
making textured polygons better looking and using smaller texture.
</p><p>
	That said, mipmapping is not implemented in the helper library yet.
</p>
<h5>Use fixed points everywhere</h5>
<p>
	This may not be the case in many OpenGL renderer, but the GP2X really does 
prefer integers to floating point values. So, use preferably fixed point 
arithmetic for all your vertexes and internal computations.
</p><p>
	Floating values are like z buffer&nbsp;: a necessary evil in some rare 
cases, an expensive and useless luxury most of the time. Integer arithmetic is 
simpler, faster, more accurate and more predictable. Only when you have no 
clue about the scale of a value do you want floating point types. This is 
probably not the case anywhere in your application.
</p>
<h3><a name="AppPort">How to port OpenGL apps</a></h3>
<p>
	This chapter gives some hints about porting existing applications to 
gpu940's OpenGL-like library. Most of these hints were gathered while porting 
<a href="http://egoboo.sourceforge.net">Egoboo</a> to the GP2X.
</p><p>
	As a general advice, choose wisely the application you attempt to port.  
Don't forget that the GPU runs on a 200MHz RISC processor with a relatively 
slow RAM compared to PCs. So, focus on well optimized applications runnable on 
old hardware. Also, remember that the OpenGL layer is nothing but a quick 
hack&nbsp;: it can draw polygons, but not much more. If your application is 
very OpenGL oriented, it will probably need parts of OpenGL that are not 
implemented. A good hint toward feasibility is&nbsp;: does the application can 
use another renderer than OpenGL (like Direct3d, glide or a custom soft 
renderer)&nbsp;? If so, it's probably independent of OpenGL most complex 
features.
</p><p>
	Now to some problems you can encounter while porting an OpenGL application
to the GP2X.
</p>
<h4>It uses a tool library (GLU, GLUT, SDL or alike)</h4>
<p>
	GLU nor GLUT exist for the GP2X. If your application use these libs for 
window management like opening/closing, you could likely replace them with 
<i>glOpen()</i>/<i>glClose()</i> calls.
</p><p>
	It it deals with user input, well, you should rewrite this part anyway.
</p><p>
	If it deals with camera positioning or mesh creation, you can easily 
rip-off the required functions from the tool library source code and include 
them readily in the application code.
</p><p>
	If it deals with menu management, you are out of luck.
</p><p>
	If the application uses SDL GL wrapper, like Egoboo does, you may want 
still be able to use SDL for user input, sound, etc... In this case, you can 
remove the <i>SDL_INIT_VIDEO</i> flag from <i>SDL_Init()</i> call... or leave 
it. Just make sure that SDL blitting functions are not used, and replace 
call to <i>SDL_GL_SwapBuffers()</i> by <i>glSwapBuffers()</i>.
</p>
<h4>It uses very big&nbsp;/ small floats</h4>
<p>
	Conversion functions from <i>gl_float.h</i> can lead to serious troubles 
when input float values are not representable in <i>GLfixed</i> type. For example, 
<i>1e-15</i> and <i>1e+8</i> are not. And some values, while representable, 
will still yield to incorrect results (an intermediate result might not be 
representable in a <i>GLfixed</i>). In general, good values are between
<i>1e-2</i> and <i>1e2</i>, which is not much.
</p><p>
	Dealing with this problem requires some maths. You need to find out what 
scale to apply to all incoming vertexes, then use the reverse scaling onto 
the <i>MODELVIEW</i> matrix. Look at the <i>RESCALE</i> macro and <i>mScale</i> 
variable in Egoboo2x for an example on how to apply this trick.
</p>
<h4>Blending is all wrong&nbsp;!</h4>
<p>
	Guessing a good shift pair for the <i>blend_coef</i> facet parameter from 
OpenGL blending settings and alpha parameters is an exercise that has good 
chances to fail. So, blending effects may not look as expected.
</p><p>
	In order to obtain a similar rendering from the original application, you 
may need to tweak the blending function and alpha values until the libGL 
library choose a <i>blend_coef</i> that better suit your need.
</p><p>
	Future versions of gpu940 may change considerably in this area.
</p>
</body>
</html>
